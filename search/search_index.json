{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Proyecto: Sedes en USA","text":""},{"location":"#descripcion","title":"Descripci\u00f3n","text":"<p>Este proyecto conecta un broker MQTT con una base de datos InfluxDB. El objetivo es recibir datos meteorol\u00f3gicos de diferentes ciudades de Estados Unidos publicados en MQTT, validar los datos y enviarlos a InfluxDB para su almacenamiento y posteriormente hacer un an\u00e1lisis con Grafana. Utilizamos varios publicadores para enviar los datos y un suscriptor para procesarlos y almacenarlos.</p>"},{"location":"#objetivo","title":"Objetivo","text":"<p>Este proyecto busca facilitar el monitoreo de datos meteorol\u00f3gicos en tiempo real, permitiendo an\u00e1lisis visuales efectivos para toma de decisiones en diferentes ciudades de Estados Unidos.</p>"},{"location":"#fases","title":"FASES","text":"<p>1. Captura de datos: </p> <ol> <li>Hemos explorado unos Dataframes de Kaggle sobre generadores e\u00f3licos, hemos analizado que datos pueden aportar m\u00e1s valor, y hemos generado un archivo csv nuevo con los datos de la ciudad. </li> <li>Hemos generado un archivo de python para generar datos sint\u00e9ticos.</li> </ol> <p>2. Env\u00edo:  Para el env\u00edo utilizados MQTT, lo que hacemos es mandar desde los archivos de Ciudad.py, de la carpeta publishers. Empleamos el qos2, Quality of Service 2, para conectarse al broker mqtt luego se mandan los datos al subscriptor.py.</p> <p>3. Validaci\u00f3n:  Antes de la almacenar los datos, se validan los datos que  los suscriptores no contentan ni valores nulos, vac\u00edos o que no sean Outliers. Tambi\u00e9n, se comprueba el formato del mensaje, para m\u00e1s detalle clicke aqu\u00ed.</p> <p>4. Persistencia de datos:</p> <p></p> <p>Para la persistencia de datos, en el archivo de subscriptor primero hemos formateado el formato del mensaje recibido para que se pueda enviar a Influx correctamente. Luego, hemos empleado la API-REST de InfluxDB le hemos indicado la acci\u00f3n que quer\u00edamos hacer, escribir, y le hemos pasado nuestro token para validarnos.</p>"},{"location":"#funcionamiento-del-envio-de-datos","title":"Funcionamiento del env\u00edo de datos","text":"<p> Esta imagen muestra el funcionamiento del proyecto desde el env\u00edo hasta la persistencia del sistema.</p> <p>5. Visualizaci\u00f3n de datos:  Para la visualizaci\u00f3n de datos en Grafana, hemos hecho una query de los datos a visualizar a InfluxDB, le hemos pasado nuestro token para que nos valide.</p>"},{"location":"#instalacion","title":"Instalaci\u00f3n","text":"<p>Clonar el repositorio:</p> <pre><code>git clone &lt;https://github.com/rdo164/XR-CTR.git&gt; \n</code></pre> <p>Instalar las dependencias:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<ul> <li>subscritor.py: Script principal que establece la conexi\u00f3n con el broker MQTT y procesa los mensajes recibidos.</li> <li>requirements.txt: Archivo con las dependencias necesarias para ejecutar el proyecto.</li> <li>certs/: Directorio que contiene los certificados necesarios para la comunicaci\u00f3n segura con el broker MQTT.</li> <li>publishers: Directorio con los los archivos de publicaci\u00f3n.<ul> <li>LVegas.csv, Portland.csv, Dallas.csv, Denver.csv, Seattle.csv : Archivos CSV con datos de ejemplo para la publicaci\u00f3n en MQTT.</li> <li>NewYork.py: Script para generar y publicar datos sint\u00e9ticos en MQTT.</li> </ul> </li> </ul>"},{"location":"#requisitos","title":"Requisitos","text":"<ul> <li>Python 3.7 o superior</li> <li>pip (gestor de paquetes de Python)</li> <li>Conexi\u00f3n a internet para instalar las dependencias</li> <li>Certificados de seguridad para el broker MQTT (ubicados en ./certs/)</li> <li>Instalaci\u00f3n de dos2unix</li> </ul>"},{"location":"#archivos-principales","title":"Archivos Principales","text":"<p>subscritor.py</p> <p>Este script principal se encarga de:</p> <ul> <li>Conectarse al broker MQTT.</li> <li>Suscribirse a un t\u00f3pico (home/#).</li> <li>Procesar los mensajes recibidos.</li> <li>Validar los datos.</li> <li>Enviar los datos a InfluxDB.</li> </ul> <p>NewYork.py</p> <p>Este script genera datos sint\u00e9ticos y los publica en el broker MQTT. Es \u00fatil para pruebas y desarrollo.</p> <p>Ciudad.py</p> <p>Este script recorre los datos de los archivos csv correpondientes a su ciudad.</p>"},{"location":"#certificados","title":"Certificados","text":"<p>Los certificados necesarios para la comunicaci\u00f3n segura con el broker MQTT deben estar ubicados en el directorio ./certs/:</p> <ul> <li>ca.crt: Certificado de la autoridad certificadora.</li> <li>LasVegas.crt, LasVegas.key: Certificados y claves para la ciudad de Las Vegas.</li> <li>Portland.crt, Portland.key: Certificados y claves para la ciudad de Portland.</li> <li>Dallas.crt, Dallas.key: Certificados y claves para la ciudad de Dallas.</li> <li>Denver.crt, Denver.key: Certificados y claves para la ciudad de Denver.</li> <li>Seattle.crt, Seattle.key: Certificados y claves para la ciudad de Seattle</li> <li>NewYork.crt, NewYork.key: Certificados y claves para datos sint\u00e9ticos.</li> </ul> <p>Para generar los certificados autom\u00e1ticamente acceder mediante certs, al archivo certificados.sh:</p> <p>En caso de tener dos2unix instalado, emplea los siguientes comandos, sino instalalo:</p> <pre><code> dos2unix certificados.sh\n</code></pre> <pre><code> bash certificados.sh\n</code></pre>"},{"location":"#ejecucion","title":"Ejecuci\u00f3n","text":"<p>Ejecuci\u00f3n del Script Principal</p> <p>Para iniciar la conexi\u00f3n con el broker MQTT y comenzar a procesar los mensajes:</p> <pre><code>docker-compose up -d \n</code></pre> <pre><code>python3 subscritor.py\n</code></pre>"},{"location":"#1-publicar-datos-desde-csv","title":"1. Publicar Datos desde CSV","text":"<p>Para publicar datos desde los archivos CSV LVegas.csv, Portland.csv, Dallas.csv, Denver.csv, Seattle.csv:</p> <pre><code>python3 Portland.py \npython3 Denver.py\npython3 Dallas.py\npython3 Seattle.py \npython3 LasVegas.py\n</code></pre> <p>Generar y Publicar Datos Sint\u00e9ticos</p> <p>Para generar y publicar datos sint\u00e9ticos:</p> <pre><code> python3 NewYork.py\n</code></pre>"},{"location":"#estructura-del-mensaje-mqtt","title":"Estructura del Mensaje MQTT","text":"<p>Los mensajes publicados en MQTT deben seguir el siguiente formato:</p> <p>Temperatura: , Tiempo: , Direccion_viento: , Velocidad_viento:  <p>Ejemplo:</p> <p>Temperatura: 25.5, Tiempo: scattered clouds, Direccion_viento: 180, Velocidad_viento: 5.5</p>"},{"location":"#3-validacion-de-datos","title":"3. Validaci\u00f3n de Datos","text":"<p>El script valida los datos recibidos seg\u00fan los siguientes criterios:</p> <ul> <li>Temperatura debe estar entre 0 y 50 grados Celsius.</li> <li>Direccion_viento debe estar entre 0 y 360 grados.</li> <li>Velocidad_viento debe estar entre 0 y 100 m/s.</li> </ul>"},{"location":"#2-envio-de-datos","title":"2. Env\u00edo de Datos","text":"<p>Para el env\u00edo de datos mediante MQTT utilizando certificados TLS/SSL, hemos implementado el uso de SAN (Subject Alternative Name). Mediante SAN, vinculamos el certificado a una IP espec\u00edfica. </p> <p>Para el servidor, hemos utilizado la IP 0.0.0.0 para que pueda escuchar desde diferentes IPs, mientras que cada cliente tiene su IP espec\u00edfica; en nuestro caso, todos los publicadores tienen las IPs 192.168.208.x. </p> <p>En el archivo de configuraci\u00f3n de Mosquitto (mosquitto.conf), tambi\u00e9n hemos a\u00f1adido la directiva bind_address 0.0.0.0 para que el contenedor del broker MQTT pueda escuchar desde cualquier IP. Adem\u00e1s, en cada certificado de los publicadores, hemos incluido el nombre de la ciudad, lo que nos permite identificar en todo momento qui\u00e9n est\u00e1 conectado.</p>"},{"location":"#4-persistencia-de-datos","title":"4. Persistencia de Datos","text":"<p>De cara a la persistencia de datos del proyecto, hemos empleado InfluxDB porque los datos que vamos a enviar son series temporales. Para el env\u00edo hemos utilizado la API de influxDB para escribir los datos, y luego posteriormente desde el Data Explorer comprobar si los datos se hab\u00edan enviado correctamente.</p> <p>Sobre los datos la clasificaci\u00f3n de los datos a la hora de insertarlos es la siguiente:</p> <ul> <li>El Bucket lo hemos llamado tiempo.</li> <li>El measurement = viento.</li> <li>Y la etiqueta la localizaci\u00f3n del lugar, para que luego de cara a la visualizaci\u00f3n tengamos la informaci\u00f3n clasificada. Tambi\u00e9n cabe la posibilidad de que el cliente quiera visualizar diferentes localizaciones.</li> </ul>"},{"location":"#5visualizacion-de-datos","title":"5.Visualizaci\u00f3n de datos.","text":"<p>Para la visualizaci\u00f3n de los datos, hemos empleado InfluxDB y Grafana, que en conjunto permiten una visualizaci\u00f3n efectiva  1. Exploraci\u00f3n de Datos en InfluxDB:</p> <ol> <li>Utilizando el Data Explorer de InfluxDB, es posible verificar que los datos se est\u00e1n enviando y almacenando correctamente. Esto asegura que los datos est\u00e1n disponibles para su an\u00e1lisis y visualizaci\u00f3n. Integraci\u00f3n con Grafana Configuraci\u00f3n de Grafana:</li> </ol> <p>Data Source: Se ha configurado InfluxDB como una fuente de datos en Grafana, proporcionando las credenciales y la URL de conexi\u00f3n.</p> <p>Dashboards: Hemos creado dashboards espec\u00edficos para visualizar los datos meteorol\u00f3gicos de cada ciudad. Cada dashboard puede incluir gr\u00e1ficos de series temporales para la temperatura, direcci\u00f3n del viento y velocidad del viento.</p>"}]}